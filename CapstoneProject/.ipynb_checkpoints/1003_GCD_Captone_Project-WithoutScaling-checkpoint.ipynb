{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"360\" />\n",
    "\n",
    "<center><h2>GCD Capstone Project : Employee Retention</h2></center><br>\n",
    "<center><h3>Group ID : 1003</h3></center>\n",
    "<center><h3>Liza Mohanty <liza.mohanty01@gmail.com>; Sudeep Raj <sudeep48raj93@gmail.com>; Raasik Ravindran <raasik1994@outlook.com>; Sudhir Takke <sudhir.takke@gmail.com>; Sibraj <sibrajb@gmail.com>; AVS Aditya <avs.aditya@gmail.com> </h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "1. [Problem Statement](#section1)<br>\n",
    "2. [Data Loading and Description](#section2)<br>\n",
    "3. [Exploratory Data Analysis](#section3)<br>\n",
    "    a.[Examine the Data](#section301)<br>\n",
    "    b.[Data pre-processing and cleaning](#section302)<br>\n",
    "    c.[EDA and Inferences](#section303)<br>\n",
    "4. [Feature Engineering](#section4)<br>\n",
    "5. [Machine Learning Models](#section5)<br>\n",
    "    a.[Build models](#section501)<br>\n",
    "    b.[Evaluate models and finalize the model](#section502)<br>\n",
    "    c.[Fit and tune models with cross-validation](#section503)<br>\n",
    "    d.[Declare hyper-parameters to tune the models](#section504)<br>\n",
    "    e.[Feature Importance](#section505)<br>\n",
    "6. [Predicting the Unknown](#section6)<br>\n",
    "    a.[Format the unseen data as per our model](#section601)<br>\n",
    "    b.[Run the model through the given data](#section602)<br>\n",
    "    c.[Publishing the results](#section603)<br>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Problem Statement\n",
    "\n",
    "Predict whether or not an employee would stay given the data of employees at a company.\n",
    "\n",
    "Your client for this project is the HR Department at a software company.\n",
    "\n",
    "    They want to try a new initiative to retain employees.\n",
    "    The idea is to use data to predict whether an employee is likely to leave.\n",
    "    Once these employees are identified, HR can be more proactive in reaching out to them before it's too late.\n",
    "    They only want to deal with the data that is related to permanent employees.\n",
    "\n",
    "Current Practice\n",
    "Once an employee leaves, he or she is taken an interview with the name \"exit interview\" and shares reasons for leaving. The HR Department then tries and learns insights from the interview and makes changes accordingly.\n",
    "\n",
    "This suffers from the following problems:\n",
    "\n",
    "    This approach is that it's too haphazard. The quality of insight gained from an interview depends heavily on the skill of the interviewer.\n",
    "    The second problem is these insights can't be aggregated and interlaced across all employees who have left.\n",
    "    The third is that it is too late by the time the proposed policy changes take effect.\n",
    "\n",
    "The HR department has hired you as data science consultants. They want to supplement their exit interviews with a more proactive approach.\n",
    "\n",
    "\n",
    "### Your Role\n",
    "\n",
    "    You are given datasets of past employees and their status (still employed or already left).\n",
    "    Your task is to build a classification model using the datasets.\n",
    "    Because there was no machine learning model for this problem in the company, you don’t have quantifiable win condition. \n",
    "    You need to build the best possible model.\n",
    "\n",
    "\n",
    "### Problem Specifics\n",
    "    Deliverable: Predict whether an employee will stay or leave.\n",
    "    Machine learning task: Classification\n",
    "    Target variable: Status (Employed/Left)\n",
    "    Win condition: N/A (best possible model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Description\n",
    "\n",
    "\n",
    "\n",
    "The Business Intelligence Analysts of the Company provided you three datasets that contain information about past employees and their status (still employed or already left).\n",
    "\n",
    "    department_data\n",
    "\n",
    "    This dataset contains information about each department. The schema of the dataset is as follows:\n",
    "        dept_id – Unique Department Code\n",
    "        dept_name – Name of the Department\n",
    "        dept_head – Name of the Head of the Department\n",
    "\n",
    "    employee_details_data\n",
    "\n",
    "    This dataset consists of Employee ID, their Age, Gender and Marital Status. The schema of this dataset is as follows:\n",
    "        employee_id – Unique ID Number for each employee\n",
    "        age – Age of the employee\n",
    "        gender – Gender of the employee\n",
    "        marital_status – Marital Status of the employee\n",
    "\n",
    "    employee_data\n",
    "\n",
    "    This dataset consists of each employee’s Administrative Information, Workload Information, Mutual Evaluation Information and Status.\n",
    "\n",
    "    Target variable\n",
    "        status – Current employment status (Employed / Left)\n",
    "\n",
    "    Administrative information\n",
    "        department – Department to which the employees belong(ed) to\n",
    "        salary – Salary level with respect to rest of their department\n",
    "        tenure – Number of years at the company\n",
    "        recently_promoted – Was the employee promoted in the last 3 years?\n",
    "        employee_id – Unique ID Number for each employee\n",
    "\n",
    "    Workload information\n",
    "        n_projects – Number of projects employee has worked on\n",
    "        avg_monthly_hrs – Average number of hours worked per month\n",
    "\n",
    "    Mutual evaluation information\n",
    "        satisfaction – Score for employee’s satisfaction with the company (higher is better)\n",
    "        last_evaluation – Score for most recent evaluation of employee (higher is better)\n",
    "        filed_complaint – Has the employee filed a formal complaint in the last 3 years?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_output\n",
    "\n",
    "# Database \n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from collections import Counter\n",
    "\n",
    "# ML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import the required packages from sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, f1_score, accuracy_score, precision_score, recall_score, precision_recall_fscore_support, roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV,train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rc(\"font\", size=14)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Source\n",
    "Download the DataSets onto Python by connecting to the below provided MySQL instance.\n",
    "                      \n",
    "    host        'cpanel.insaid.co' #'projects.insaid.co'\n",
    "    user \t   'student'\n",
    "    passwd      'student'\n",
    "    database    'Capstone2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = ['department_data', 'employee_details_data', 'employee_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to mySQL, fetch employee data, employee details data and department data into respective dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mySQLconnection = mysql.connector.connect(host='cpanel.insaid.co',#host='projects.insaid.co',\n",
    "                             database='Capstone2',\n",
    "                             user='student',\n",
    "                             password='student')\n",
    "    \n",
    "    if mySQLconnection.is_connected():\n",
    "        db_Info = mySQLconnection.get_server_info()\n",
    "        print(\"Connected to MySQL database... MySQL Server version on \",db_Info)\n",
    "        cursor = mySQLconnection.cursor()\n",
    "        cursor.execute(\"select database();\")\n",
    "        database = cursor.fetchone()\n",
    "        print (\"Your connected to - \", database)\n",
    "        # for table in tables:\n",
    "            # cursor.execute(sql_select_Query)\n",
    "            # records = cursor.fetchall()\n",
    "            # print (records)\n",
    "        sql_select_Query = \"select * from department_data\"\n",
    "        SQL_Query = pd.read_sql_query(sql_select_Query, mySQLconnection)\n",
    "        df_departmentData = pd.DataFrame(SQL_Query)\n",
    "        print(df_departmentData.head(3))\n",
    "        \n",
    "        sql_select_Query = \"select * from employee_details_data\"\n",
    "        SQL_Query = pd.read_sql_query(sql_select_Query, mySQLconnection)\n",
    "        df_employeeDetailsData = pd.DataFrame(SQL_Query)\n",
    "        print(df_employeeDetailsData.head(3))\n",
    "\n",
    "        sql_select_Query = \"select * from employee_data\"\n",
    "        SQL_Query = pd.read_sql_query(sql_select_Query, mySQLconnection)\n",
    "        df_employeeData = pd.DataFrame(SQL_Query)\n",
    "        print(df_employeeData.head(3))\n",
    "except Error as e :\n",
    "    print (\"Error while connecting to MySQL\", e)\n",
    "finally:\n",
    "    #closing database connection.\n",
    "    if(mySQLconnection .is_connected()):\n",
    "        mySQLconnection.close()\n",
    "        print(\"MySQL connection is closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the number of records and features/columns in each of the dataframes: department,employee, employee details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_departmentData.to_csv('department_data_sql.csv')\n",
    "df_employeeDetailsData.to_csv('employee_details_data_sql.csv')\n",
    "df_employeeData.to_csv('employee_data_sql.csv')\n",
    "print (df_departmentData.shape)\n",
    "print (df_employeeDetailsData.shape)\n",
    "print (df_employeeData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section301></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Examine the data\n",
    "Now let us examine the data for general observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_departmentData.head(3))\n",
    "print(df_departmentData.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_employeeDetailsData.head(3))\n",
    "print(df_employeeDetailsData.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df_employeeData.head(3))\n",
    "print(df_employeeData.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_employeeData['employee_id'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now we can see that the min employee Id is 0 that means there are such records with an invalid employee id.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employeeData[df_employeeData['employee_id']<=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the records with employee Id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employeeData.drop(df_employeeData[df_employeeData['employee_id']<=0].index,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employeeData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_employeeData['avg_monthly_hrs'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The minimum monthly hours are 49 and max is 310 but the average is 200 and the Q1 is around 155.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_departmentData['dept_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employeeData['department'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace Incorrect Data -IT with proper name. This is very important to avoid invalid elements during the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employeeData['department'].replace({'-IT': 'D00-IT'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employeeData['department'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join with employee details dataset to get marital status, age, gender information for each employee in employee dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_empData = pd.merge(df_employeeData, df_employeeDetailsData, how='left', on=\"employee_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_empData.head())\n",
    "print (df_empData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join employee dataset with department dataset to get department details for the department each employee belongs to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_empData, df_departmentData, how='left', left_on='department', right_on='dept_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df.head())\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('department', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('Employee_original_data.csv')\n",
    "print (df.head())\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section302></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Data pre-processing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()[df.isnull().sum() !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()[df.isnull().sum() !=0]\n",
    "missing = pd.DataFrame(missing.reset_index())\n",
    "missing.rename(columns={'index':'features',0:'missing_count'}, inplace = True)\n",
    "missing['missing_count_percentage'] = ((missing['missing_count'])/df.shape[0])*100\n",
    "plt.figure()\n",
    "sns.barplot(y = missing['features'], x = missing['missing_count_percentage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that the number of missing elements for *filed_complaint* and *recently_promoted* columns is too high. It is better to drop them for the sake of having clean data. Also, we can drop the records with missing department details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['filed_complaint','recently_promoted'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop records with missing elements\n",
    "df.dropna(subset=['dept_id', 'dept_name', 'dept_head'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute the missing values - Analyze the outliers to choose either Median or Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(figsize=(15,10))\n",
    "sns.boxplot(df['last_evaluation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_evaluation has no outliers - Mean should do the job here\n",
    "df['last_evaluation'].fillna(df['last_evaluation'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(figsize=(15,10))\n",
    "sns.boxplot(df['satisfaction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# satisfaction has no outliers - Mean should do the job here\n",
    "df['satisfaction'].fillna(df['satisfaction'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(figsize=(15,10))\n",
    "sns.boxplot(df['tenure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tenure has outliers - Median should do the job here\n",
    "df['tenure'].fillna(df['tenure'].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()[df.isnull().sum() !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "df[df.duplicated()].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicates\n",
    "df.drop_duplicates(inplace = True,keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Challenges "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Few records in the employee dataset have incorrect department values \"-IT\"\n",
    "        a. Get the correct department value from the Department data set and update the employee records with the correct department value D00-IT \t\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "    2. Following fields from the employee data set have missing values:\n",
    "        recently_promoted    97.90 %\n",
    "        filed_complaint      85.54%\n",
    "        last_evaluation      10.51%\n",
    "        department            5.00%\n",
    "        tenure                1.06%\n",
    "        satisfaction          1.06%\"\t\t\t\n",
    "        a. Drop the columns/fields with high number of missing values i.e.recently_promoted and filed_complaint.\n",
    "        b. Delete the records with missing /null values of department since the percentages of records with these missing values are very less.\n",
    "        c. We have visualize the box plot for last_evaluation and satisfaction columns, value ranges between 0 and 1. Missing records present is 1.06%. We have replaced the same with the mean value.\n",
    "        d. Similarly we have visualize the box plot for tenure column and found outliers, value ranges between 2 and 10. Missing records present is 1.06%. We have replaced the same with the median value because we need integer not float.\n",
    "\t\t\t\t\t\n",
    "    3. Employee personal details i.e. age , gender, marital_status and department details i.e. name and dept head details are in separate data sets.\t\t\t\n",
    "        a. Perform a left merge/join to get all the personal details as well as the department details into a single dataframe. This gives a complete employee data set.\t\n",
    "\t\t\t\t\t\t\t\t\t\n",
    "    4. Few records (5 records) in the employee data set are present with 0 as the employee ID.\t\t\t\n",
    "        a. Delete the invalid records\t\n",
    "\t\t\t\t\n",
    "    5. There are few duplicate records (29 records) with the same empoyee id and employee details in the employee dataset.\t\t\ta. Delete the duplicate records.\t\n",
    "\t\t\t\t\t\n",
    "    6. The final employee dataset has the following columns:\n",
    "        a. avg monthly hrs   \n",
    "        b. department (replace dept id with the dept name)     \n",
    "        c. last evaluation    \n",
    "        d. no of projects        \n",
    "        e. salary            \n",
    "        f. satisfaction       \n",
    "        g. status (target variable)\n",
    "        h. tenure\n",
    "        i. age               \n",
    "        j. gender            \n",
    "        k. marital status \n",
    "\n",
    "    7. Outliers : There are no outliers\t\t\t\n",
    "        a. No action needs to be taken \t\n",
    "\n",
    "    8. Following are the categorical features :\n",
    "        department, salary, status, gender, marital_status\t\n",
    "        a. Use label encoding to change the following categorical features to a numerical format:\n",
    "            salary (low-1, medium-2, high-0). Here , salary is an ordinal feature\n",
    "            status (left-1, employed-0)\n",
    "            gender (male-1, female-0)\n",
    "            marital status(married-0,unmarried-1)\n",
    "        b. Apply get_dummies on the department feature and get the employee updated dataset with 20 features.\"\t\n",
    "\t\t\t\t\t\t\t\t\t\n",
    "    9. Following are the numerical features:\n",
    "        average monthly hrs, last evaluation,no of projects , tenure, age\n",
    "        a. Apply scaling process to the following features to bring them to the same range:\n",
    "            average monthly hrs, no of projects, tenure and age\"\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section303></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. EDA and Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\"gender\", \"marital_status\", \"salary\", \"status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categoricals(data):\n",
    "    ncols = len(data.columns)\n",
    "    fig = plt.figure(figsize=(5 * 5, 5 * (ncols // 5 + 1)))\n",
    "    for i, col in enumerate(data.columns):\n",
    "        cnt = Counter(data[col])\n",
    "        keys = list(cnt.keys())\n",
    "        vals = list(cnt.values())\n",
    "        plt.subplot(ncols // 5 + 1, 5, i + 1)\n",
    "        plt.bar(range(len(keys)), vals, align=\"center\")\n",
    "        plt.xticks(range(len(keys)), keys)\n",
    "        plt.xlabel(col, fontsize=18)\n",
    "        plt.ylabel(\"frequency\", fontsize=18)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_categoricals(df[CATEGORICAL_COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTINUOUS_COLUMNS = [\"age\", \"avg_monthly_hrs\", \"last_evaluation\", \"n_projects\", \"satisfaction\", \"tenure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histgrams(data):\n",
    "    ncols = len(data.columns)\n",
    "    fig = plt.figure(figsize=(5 * 5, 5 * (ncols // 5 + 1)))\n",
    "    for i, col in enumerate(data.columns):\n",
    "        X = data[col].dropna()\n",
    "        plt.subplot(ncols // 5 + 1, 5, i + 1)\n",
    "        plt.hist(X, bins=20, alpha=0.5, \\\n",
    "                 edgecolor=\"black\", linewidth=2.0)\n",
    "        plt.xlabel(col, fontsize=18)\n",
    "        plt.ylabel(\"frequency\", fontsize=18)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_histgrams(df[CONTINUOUS_COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis1 = plt.subplots(1,1,figsize=(10,5))\n",
    "sns.countplot(x='status',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the count of people\n",
    "leftcounts=df['status'].value_counts()\n",
    "print(leftcounts)\n",
    "\n",
    "# Using matplotlib pie chart and label the pie chart\n",
    "plt.pie(leftcounts,labels=['Employed','Left']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(df, hue=\"status\",aspect=4, hue_order=['Employed', 'Left'])\n",
    "facet.map(sns.kdeplot,'age')\n",
    "facet.set(xlim=(0, df['age'].max()))\n",
    "facet.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df[\"age\"],bins=10,kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(df, hue=\"status\",aspect=4, hue_order=['Employed', 'Left'])\n",
    "facet.map(sns.kdeplot,'last_evaluation')\n",
    "facet.set(xlim=(0, df['last_evaluation'].max()))\n",
    "facet.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df[\"last_evaluation\"],bins=10,kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis1 = plt.subplots(1,1,figsize=(15,5))\n",
    "sns.countplot(x='gender', hue=\"status\", data=df, ax=axis1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis1 = plt.subplots(1,1,figsize=(15,5))\n",
    "sns.countplot(x='marital_status', hue=\"status\", data=df, ax=axis1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axis1 = plt.subplots(1,1,figsize=(15,5))\n",
    "sns.countplot(x='salary', hue=\"status\", data=df, ax=axis1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis1 = plt.subplots(1,1,figsize=(15,5))\n",
    "sns.countplot(x='tenure', hue=\"status\", data=df, ax=axis1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(df, hue=\"status\",aspect=4, hue_order=['Employed', 'Left'])\n",
    "facet.map(sns.kdeplot,'satisfaction')\n",
    "facet.set(xlim=(0, df['satisfaction'].max()))\n",
    "facet.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(df, hue=\"status\",aspect=4, hue_order=['Employed', 'Left'])\n",
    "facet.map(sns.kdeplot,'last_evaluation')\n",
    "facet.set(xlim=(0, df['last_evaluation'].max()))\n",
    "facet.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(df, hue=\"salary\",aspect=4, hue_order=['low', 'medium', 'high'])\n",
    "facet.map(sns.kdeplot,'last_evaluation')\n",
    "facet.set(xlim=(0, df['last_evaluation'].max()))\n",
    "facet.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(df, hue=\"salary\",aspect=4, hue_order=['low', 'medium', 'high'])\n",
    "facet.map(sns.kdeplot,'satisfaction')\n",
    "facet.set(xlim=(0, df['satisfaction'].max()))\n",
    "facet.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(df, hue=\"salary\",aspect=4, hue_order=['low', 'medium', 'high'])\n",
    "facet.map(sns.kdeplot,'avg_monthly_hrs')\n",
    "facet.set(xlim=(0, df['avg_monthly_hrs'].max()))\n",
    "facet.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(df, hue=\"status\",aspect=4, hue_order=['Employed', 'Left'])\n",
    "facet.map(sns.kdeplot,'n_projects')\n",
    "facet.set(xlim=(0, df['n_projects'].max()))\n",
    "facet.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(df, hue=\"status\",aspect=4, hue_order=['Employed', 'Left'])\n",
    "facet.map(sns.kdeplot,'tenure')\n",
    "facet.set(xlim=(0, df['tenure'].max()))\n",
    "facet.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(df, hue=\"status\",aspect=4, hue_order=['Employed', 'Left'])\n",
    "facet.map(sns.kdeplot,'avg_monthly_hrs')\n",
    "facet.set(xlim=(0, df['avg_monthly_hrs'].max()))\n",
    "facet.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_df = df[(df['status'] == 'Left')]\n",
    "employed_df = df[(df['status'] != 'Left')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a figure instance, and the two subplots\n",
    "fig = plt.figure(figsize=(20,17))\n",
    "ax1 = fig.add_subplot(521)\n",
    "ax2 = fig.add_subplot(522)\n",
    "ax3 = fig.add_subplot(523)\n",
    "ax4 = fig.add_subplot(524)\n",
    "ax5 = fig.add_subplot(525)\n",
    "ax6 = fig.add_subplot(526)\n",
    "ax7 = fig.add_subplot(527)\n",
    "ax8 = fig.add_subplot(528)\n",
    "ax9 = fig.add_subplot(529)\n",
    "ax10 = fig.add_subplot(5,2,10)# Tell pointplot to plot on ax1 with the ax argument (satisfaction level)\n",
    "sns.distplot(left_df['satisfaction'],ax = ax1);\n",
    "sns.distplot(employed_df['satisfaction'],ax = ax2);\n",
    "sns.distplot(left_df['last_evaluation'], kde=True,ax=ax3);\n",
    "sns.distplot(employed_df['last_evaluation'], kde=True,ax=ax4);\n",
    "sns.distplot(left_df['n_projects'], kde=True,ax=ax5);\n",
    "sns.distplot(employed_df['n_projects'], kde=True,ax=ax6);\n",
    "sns.distplot(left_df['avg_monthly_hrs'], kde=True,ax=ax7);\n",
    "sns.distplot(employed_df['avg_monthly_hrs'], kde=True,ax=ax8);\n",
    "sns.distplot(left_df['tenure'], kde=True,ax=ax9);\n",
    "sns.distplot(employed_df['tenure'], kde=True,ax=ax10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights: Profile of the people who left\n",
    "\n",
    "    satisfaction: Employees who left include both satisfied and dissatisfied with their job.\n",
    "    last_evaluation: People leaving the company are majorly high and low performers. \n",
    "    So it could be that they did not get the right opportunities to work or maybe they got better opportunities elsewhere.\n",
    "    n_projects: Most people are working on 2 projects.\n",
    "    avg_montly_hrs: The average of 160 to 220 hours working people seem to be staying back. Those who are working less seem to be disconnected (lack of interest, motivation, etc.) and those working overtime seemed to be stressed.\n",
    "    tenure: 3-5 years seems to be the age range where people decide if they want to stay back or leave. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a figure instance, and the two subplots\n",
    "fig = plt.figure(figsize=(20,17))\n",
    "ax1 = fig.add_subplot(511)\n",
    "ax2 = fig.add_subplot(512)\n",
    "ax3 = fig.add_subplot(513)\n",
    "ax4 = fig.add_subplot(514)\n",
    "ax5 = fig.add_subplot(515)\n",
    "sns.distplot(df['satisfaction'],ax = ax1);\n",
    "sns.distplot(df['last_evaluation'], kde=True,ax=ax2);\n",
    "sns.distplot(df['n_projects'], kde=True,ax=ax3);\n",
    "sns.distplot(df['avg_monthly_hrs'], kde=True,ax=ax4);\n",
    "sns.distplot(df['tenure'], kde=True,ax=ax5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "role_ed_xtab = pd.crosstab(df['gender'], df['status'], normalize='index')\n",
    "sns.heatmap(role_ed_xtab, annot=True, fmt='0.0%', cmap='YlOrRd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "role_ed_xtab = pd.crosstab(df['marital_status'], df['status'], normalize='index')\n",
    "sns.heatmap(role_ed_xtab, annot=True, fmt='0.0%', cmap='YlOrRd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "role_ed_xtab = pd.crosstab(df['salary'], df['status'], normalize='index')\n",
    "sns.heatmap(role_ed_xtab, annot=True, fmt='0.0%', cmap='YlOrRd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "role_ed_xtab = pd.crosstab(df['gender'], df['marital_status'], normalize='index')\n",
    "sns.heatmap(role_ed_xtab, annot=True, fmt='0.0%', cmap='YlOrRd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "role_ed_xtab = pd.crosstab(df['gender'], df['salary'], normalize='index')\n",
    "sns.heatmap(role_ed_xtab, annot=True, fmt='0.0%', cmap='YlOrRd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "role_ed_xtab = pd.crosstab(df['marital_status'], df['salary'], normalize='index')\n",
    "sns.heatmap(role_ed_xtab, annot=True, fmt='0.0%', cmap='YlOrRd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df['gender'], df['age'])\n",
    "plt.title('Age vs Gender Box Plot', fontsize=20)      \n",
    "plt.xlabel('Gender', fontsize=16)\n",
    "plt.ylabel('Age', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df['gender'], df['last_evaluation'])\n",
    "plt.title('Evaluation vs Gender Box Plot', fontsize=20)      \n",
    "plt.xlabel('Gender', fontsize=16)\n",
    "plt.ylabel('Evaluation', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df['gender'], df['satisfaction'])\n",
    "plt.title('Satisfaction vs Gender Box Plot', fontsize=20)      \n",
    "plt.xlabel('Gender', fontsize=16)\n",
    "plt.ylabel('satisfaction', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot9():\n",
    "  temp = df.copy(deep=True)\n",
    "  num = {\"status\": {\"Left\": 1, \"Employed\": 0}}\n",
    "  temp.replace(num, inplace=True)\n",
    "  sns.catplot('tenure','status', hue= 'gender', kind='point',data=temp)\n",
    "  plt.title('Tenure vs Employment status with gender')\n",
    "plot9()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df['marital_status'], df['age'])\n",
    "plt.title('Age vs marital_status Box Plot', fontsize=20)      \n",
    "plt.xlabel('marital_status', fontsize=16)\n",
    "plt.ylabel('Age', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df['marital_status'], df['last_evaluation'])\n",
    "plt.title('Evaluation vs marital_status Box Plot', fontsize=20)      \n",
    "plt.xlabel('marital_status', fontsize=16)\n",
    "plt.ylabel('Evaluation', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df['marital_status'], df['satisfaction'])\n",
    "plt.title('Satisfaction vs marital_status Box Plot', fontsize=20)      \n",
    "plt.xlabel('marital_status', fontsize=16)\n",
    "plt.ylabel('satisfaction', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot6():\n",
    "    # Plotting Salary against satisfaction \n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.boxplot(x='satisfaction', y='salary', data=df, hue='status',palette='Set3')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Salary vs Satisfication level ')\n",
    "    plt.show()\n",
    "plot6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "role_ed_xtab = pd.crosstab(df['dept_name'], df['status'], normalize='index')\n",
    "sns.heatmap(role_ed_xtab, annot=True, fmt='0.0%', cmap='YlOrRd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "role_ed_xtab = pd.crosstab(df['dept_head'], df['status'], normalize='index')\n",
    "sns.heatmap(role_ed_xtab, annot=True, fmt='0.0%', cmap='YlOrRd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stripplot\n",
    "sns.stripplot(x='n_projects', y='avg_monthly_hrs', data=df, jitter=True, hue='status', dodge=True)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x='n_projects', y='avg_monthly_hrs', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.swarmplot(x='n_projects', y='avg_monthly_hrs', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Correlation between parameters only for those employees who have left\")\n",
    "corr = left_df.corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation between parameters only for those employees who are still employed\")\n",
    "corr = employed_df.corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights: Correlated Elements\n",
    "\n",
    "    n_projects and avg_monthly_hrs: More the projects, more the time you spend on them. This could be possibly the reason of dissatisfaction.\n",
    "    last_evaluation and avg_monthly_hrs: This indicates that the longer the monthly hours, the more likely you get a good last evaluation. \n",
    "    satisfaction and tenure: They have decent correlation\n",
    "    avg_monthly_hrs and age: Very strong correlation so indicates young people tend to work more\n",
    "    n_projects and age:Same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df,kind='scatter', hue='status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(df, hue=\"status\", height=8) \\\n",
    "    .map(plt.scatter, \"satisfaction\", \"last_evaluation\") \\\n",
    "    .add_legend();\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(df, hue=\"status\", height=4) \\\n",
    "    .map(plt.scatter, \"avg_monthly_hrs\", \"n_projects\") \\\n",
    "    .add_legend();\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axis1 = plt.subplots(1,1,figsize=(15,5))\n",
    "sns.countplot(x='dept_name', hue=\"status\", data=df, ax=axis1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments = df['dept_name'].unique()\n",
    "departments = departments[departments != 'unknown']\n",
    "departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Department  - Employees % - Dept. Attrition % - Overall Attrition %\")\n",
    "for dept in departments:\n",
    "    print (\"%-11s - %-11.2f - %-17.2f - %-6.2f\" %(dept, \\\n",
    "                                        df[(df['dept_name'] == dept)]['dept_name'].count()/df['dept_name'].count() * 100, \\\n",
    "                                        df[(df['dept_name'] == dept) & (df['status'] == \"Left\")]['dept_name'].count()/df[(df['dept_name'] == dept)]['dept_name'].count() * 100, \\\n",
    "                                        df[(df['dept_name'] == dept) & (df['status'] == \"Left\")]['dept_name'].count()/df[(df['status'] == \"Left\")]['dept_name'].count() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_count_by_department = np.zeros(len(departments))\n",
    "dept_attrition = np.zeros(len(departments))\n",
    "overall_attrition = np.zeros(len(departments))\n",
    "i = 0\n",
    "for dept in departments:\n",
    "    employee_count_by_department[i] = df[(df['dept_name'] == dept)]['dept_name'].count()/df['dept_name'].count() * 100\n",
    "    dept_attrition[i] = df[(df['dept_name'] == dept) & (df['status'] == \"Left\")]['dept_name'].count()/df[(df['dept_name'] == dept)]['dept_name'].count() * 100\n",
    "    overall_attrition[i] = df[(df['dept_name'] == dept) & (df['status'] == \"Left\")]['dept_name'].count()/df[(df['status'] == \"Left\")]['dept_name'].count() * 100\n",
    "    i += 1\n",
    "\n",
    "attrition_df = pd.DataFrame({'Employee_Percentage': employee_count_by_department, 'Dept_attrition': dept_attrition, 'Overall_attrition': overall_attrition}, index=departments)\n",
    "attrition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "attrition_df.sort_values(by='Employee_Percentage', ascending=False).plot.pie('Employee_Percentage', legend=False, autopct='%.2f%%', figsize=(8,8))\n",
    "attrition_df.sort_values(by='Dept_attrition', ascending=False).plot.pie('Dept_attrition', legend=False, autopct='%.2f%%', figsize=(8,8))\n",
    "attrition_df.sort_values(by='Overall_attrition', ascending=False).plot.pie('Overall_attrition', legend=False, autopct='%.2f%%', figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which departments whose people leave most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"dept_name\",data=df,col=\"salary\",kind=\"count\",aspect=0.5,height=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"dept_name\",data=left_df,col=\"salary\",kind=\"count\",aspect=0.5,height=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"dept_name\",data=df,col=\"gender\",kind=\"count\",aspect=0.5,height=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"dept_name\",data=left_df,col=\"gender\",kind=\"count\",aspect=0.5,height=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"dept_name\",data=df,col=\"marital_status\",kind=\"count\",aspect=0.5,height=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"dept_name\",data=left_df,col=\"marital_status\",kind=\"count\",aspect=0.5,height=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure instance, and the two subplots\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax1 = fig.add_subplot(411)\n",
    "ax2 = fig.add_subplot(412)\n",
    "ax3 = fig.add_subplot(413)\n",
    "ax4 = fig.add_subplot(414)\n",
    "\n",
    "sns.boxplot(x=\"dept_name\",y=\"satisfaction\",data=df,ax=ax1)\n",
    "sns.boxplot(x=\"dept_name\",y=\"tenure\",data=df,ax=ax2)\n",
    "sns.boxplot(x=\"dept_name\",y=\"n_projects\",data=df,ax=ax3)\n",
    "sns.boxplot(x=\"dept_name\",y=\"avg_monthly_hrs\",data=df,ax=ax4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure instance, and the two subplots\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax1 = fig.add_subplot(411)\n",
    "ax2 = fig.add_subplot(412)\n",
    "ax3 = fig.add_subplot(413)\n",
    "ax4 = fig.add_subplot(414)\n",
    "\n",
    "sns.boxplot(x=\"dept_name\",y=\"satisfaction\",data=left_df,ax=ax1)\n",
    "sns.boxplot(x=\"dept_name\",y=\"tenure\",data=left_df,ax=ax2)\n",
    "sns.boxplot(x=\"dept_name\",y=\"n_projects\",data=left_df,ax=ax3)\n",
    "sns.boxplot(x=\"dept_name\",y=\"avg_monthly_hrs\",data=left_df,ax=ax4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights: Departmental Data\n",
    "    The satisfaction level in Sales, engineering and support department is actually higher than procurement and finance. This means that probably they left because of low salary.\n",
    "    Procurement and Finance have the lowest satisfaction level with most of the quartiles located below 0.5. So that could account for the attrition in finance but not procurement team. Procurement team is handling more projects and their working hours are still in the niche region. Work Balance might be the critical factor there.\n",
    "    Marketing and product have high satisfaction levels despite high rate of quitting owing low salary.\n",
    "    One interesting observation is that married people tend to quit less compared to unmarried. This is probably that the former wants more settlement than risk taking attitude in former.\n",
    "    Also the amount of women quitting is quite alarming. This is probable due to marriage, child care, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "    1. Employee status Vs Age :\n",
    "        a. We observed that large numer of employees are from age 20 to 35 years and the attrition rate of the employees are more in the same age group.  Greater the experience less is the attrition rate.\n",
    "\t\t\t\t\t\n",
    "    2. Graph - Employee status with respect to Last Evaluation:\n",
    "        a. We observed the graph for the employees evaluated between 0.4 to 0.6 and 0.8 to 1. \n",
    "        Lesser evaluation relates to low performance and greater evaluation relates to highly skilled employees. \n",
    "        In both the scenarios attrition rate is high. Employees between 0.6 to 0.8 comes under the mid range who are stable in the organization.\n",
    "\t\t\t\t\t\n",
    "    3. Employee satisfaction is a very important role in attriton, if employee are satisfied then the attrition rate would be less. Employee having satisfaction rate 0.5 to 1 are the satisfied employees and they are working in the organization. Majority of the employees below 0.5 satisfaction rate left the organization.\n",
    "    \n",
    "    4. If we observe the tenure column, we can see that employee left the organization within a tenure of 3 to 5. More the number of years of experience, the employee stays in the organisation.\n",
    "\t\t\t\t\t\n",
    "    5. There are almost 50% of female employees as compared to the male employees. If we observe the gender graph, attrition rate of female is more. HR team should make all possibe changes to the policies for women to reduce the attrition rate by some extent for e.g. work from home facility and child care facility. Most women quit or take a break from the work. We can encourage female employees by providing good policies, work from home facility etc.\t\n",
    "\t\t\t\t\t\n",
    "    6. last_evaluation Vs satisfaction with respect to the employee status:\n",
    "       We observed three clusters. Employees are getting clubbed, they are related to each other. There is dissatisfaction among those who are performing very good as well as those performing very bad.\t\n",
    "\t\t\t\t\t\n",
    "    7. If we observe the average working hours of employees, employee who left the organization either work very less i.e. between 125 to 175 hrs and maximum from 240 to 300 hrs. For maximum working hours we can assume that the work pressure was too high  or the work was not intersingor challenging enough as per employee. Management team need to improve the work culture to facilitate good work life balance for the employees. \t\n",
    "\t\t\t\t\t\n",
    "    8. Strong positive correlation between no. of projects and avg monthly hrs: \n",
    "        a. More the no of projects, more the time an employee has spent. This could be possibly the reason of dissatisfaction.\n",
    "        b. Very strong positive correlation between last_evaluation and avg_monthly_hrs. This indicates that the longer the monthly hours, the more likely you get a good last evaluation. \n",
    "        c. So, people working on multiple projects tend to spend more time and are maybe dissatisfied due to that.\n",
    "        d. Minimize the number of projects people have to work, encourage them to share the work load, take vacations to ease the stress off them\t\n",
    "\t\t\t\t\t\n",
    "    9. The satisfaction level in Sales, engineering and support department is actually higher than procurement and finance. \n",
    "       a. This means that probably employees left because of low salary given in these departments.\n",
    "       b. Procurement and Finance have the lowest satisfaction level with most of the quartiles located below 0.5. So that could account for the attrition in finance but not procurement team. Procurement team is handling more projects and their working hours are still in the niche region. Work Balance might be the critical factor here.\n",
    "       c. Marketing and product have high satisfaction levels despite high rate of quitting owing low salary.\n",
    "       d. Salary normalization as per market standards is recommended. The salary distribution seems to uneven and would cause discrepancies.\n",
    "       e. One could also provide incentives for the best performers and keep them motivated. \n",
    "\t\t\t\t\t\n",
    "    10. One interesting observation is that married people tend to quit less compared to unmarried. This is probably that the former wants more settlement than risk taking attitude. Give more challenging opportunities to the capable ones.\t\n",
    "\t\t\t\t\t\n",
    "    11.\tAn interesting observation is that the last evaluation of married people is higher in general compared to unmarried people. But as far as satisfaction, unmarried people are doing better off. This might be due to the fact that the married people are getting less salary but have more responsiblities. Again salary correction might check this off.\t\n",
    "\t\t\t\t\t\n",
    "    12.\tThe top three departments in terms of attrition are Sales, Engineering and Support which also account for 65% of the total sales force. But a small department(5% of total work force) like Finance has the highest department level attrition of about 27%. Low salary might be the concern because their working hours seem to be in the normal range but satisfaction is pretty low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have done data pre-processing and cleaned the data for missing values, duplicates, etc. But we have fields having categorical data like marital status, gender, etc. So, it is advised to encode them to fit them better in a model. Also, normalizing the age and other continuous variables is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the department id, emp id, dept head as these columns are not required\n",
    "df_copy = df.copy()\n",
    "df = df.drop(['dept_id', 'dept_head', 'employee_id'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encode the cateorical features\n",
    "le = LabelEncoder()\n",
    "\n",
    "df['gender'] = le.fit_transform(df['gender'])\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)\n",
    "df['marital_status'] = le.fit_transform(df['marital_status'])\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)\n",
    "df['salary'] = le.fit_transform(df['salary'])\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)\n",
    "df['status'] = le.fit_transform(df['status'])\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)\n",
    "df['dept_name'] = le.fit_transform(df['dept_name'])\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Employee_encoded_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section5></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a classification problem, we would build the below models and compare them for efficiency.\n",
    "1. Logistic Regression\n",
    "2. Decision Tree Clasifier\n",
    "3. Random Forest Classifier\n",
    "4. KNN Model\n",
    "5. Naive Bayes classifier\n",
    "6. SVC Model\n",
    "7. Gradient Boosting Model\n",
    "8. XGBoost Model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section501></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['avg_monthly_hrs', 'last_evaluation', 'n_projects', 'salary', 'satisfaction', 'tenure', 'age', 'gender', 'marital_status', 'dept_name']]\n",
    "y = df.status\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "\n",
    "print(\"X Train Shape \",X_train.shape)\n",
    "print(\"Y Train Shape \",y_train.shape)\n",
    "\n",
    "print(\"X Test Shape \",X_test.shape)\n",
    "print(\"Y Test Shape \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = logreg.predict(X_test)\n",
    "\n",
    "# Accuracy score\n",
    "logreg_ac = accuracy_score(y_test, y_pred_test)*100\n",
    "print('Accuracy is :',logreg_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "def print_confusion_matrix_employee(y_actual, y_pred, modelName):\n",
    "    cnf_matrix = confusion_matrix(y_actual, y_pred)\n",
    "    cnf_mat_df = pd.DataFrame(cnf_matrix)\n",
    "    cnf_mat_df.index = ['Actual Employee','Actual Left']\n",
    "    cnf_mat_df.columns = ['Predicted Employee','Predicted Left']\n",
    "    print(\"Confusion matrix for %s model\" %modelName)\n",
    "    print(cnf_mat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_matrix_employee(y_test, y_pred_test, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC ROC Curve\n",
    "probs = logreg.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "print(metrics.classification_report(y_test,y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Tree Clasifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtmodel = tree.DecisionTreeClassifier(random_state = 0)\n",
    "dtmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test1 = dtmodel.predict(X_test)\n",
    "\n",
    "DecisionT_Accuracy = accuracy_score(y_test,y_pred_test1)*100\n",
    "print('Accuracy:', DecisionT_Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_matrix_employee(y_test, y_pred_test1, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "print(metrics.classification_report(y_test,y_pred_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100,random_state=0)\n",
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test2 = rfc.predict(X_test)\n",
    "\n",
    "rfc_ac = accuracy_score(y_test,y_pred_test2)*100\n",
    "print('Accuracy is :',rfc_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_matrix_employee(y_test, y_pred_test2, \"Random Forest Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "print(metrics.classification_report(y_test,y_pred_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors =1 )\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test3 = knn.predict(X_test)\n",
    "\n",
    "knn_ac = accuracy_score(y_test,y_pred_test3)*100\n",
    "print('Accuracy is :',knn_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_matrix_employee(y_test, y_pred_test3, \"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "print(metrics.classification_report(y_test,y_pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test4 = nb.predict(X_test)\n",
    "\n",
    "nb_ac = accuracy_score(y_test,y_pred_test4)*100\n",
    "print('Accuracy is :',nb_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_matrix_employee(y_test, y_pred_test4, \"Naive Bayes Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "print(metrics.classification_report(y_test,y_pred_test4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. SVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(random_state = 0)\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test5 = svc.predict(X_test)\n",
    "\n",
    "svc_ac = accuracy_score(y_test,y_pred_test5)*100\n",
    "print('Accuracy is :',svc_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_matrix_employee(y_test, y_pred_test5, \"SVC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "print(metrics.classification_report(y_test,y_pred_test5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model= GradientBoostingClassifier(n_estimators=100,learning_rate=0.1,random_state=1)\n",
    "gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test6 = gb_model.predict(X_test)\n",
    "\n",
    "gb_ac = accuracy_score(y_test,y_pred_test6)*100\n",
    "print('Accuracy is :',gb_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_matrix_employee(y_test, y_pred_test6, \"Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "print(metrics.classification_report(y_test,y_pred_test6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model=XGBClassifier(random_state=1,learning_rate=0.10)\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test7=xgb_model.predict(X_test)\n",
    "\n",
    "xgb_ac = accuracy_score(y_test,y_pred_test7)*100\n",
    "print('Accuracy is :',xgb_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_matrix_employee(y_test, y_pred_test7, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "print(metrics.classification_report(y_test,y_pred_test7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section502></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Evaluate models and finalize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ComparisonTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Sr no.| Model | Accuracy score | Precision | Recall | f1-score |\n",
    "| - | - | - | - | - | - |\n",
    "| 1. | Logistic Regression  | 77.74 | 0.75 | 0.78 | 0.75 | \n",
    "| 2. | Decision Tree Clasifiers | 96.72 | 0.97 | 0.97 | 0.97 |\n",
    "| 3. | Random Forest Classifier | 97.72 | 0.98 | 0.98 | 0.98 |\n",
    "| 4. | KNN Model | 94.37 | 0.94 | 0.94 | 0.94 |\n",
    "| 5. | Naive Bayes classifier | 81.65 | 0.80 | 0.82 | 0.81 |\n",
    "| 6. | SVC Model | 82.70 | 0.83 | 0.83 | 0.80 |\n",
    "| 7. | Gradient Boosting Model | 96.87 | 0.97 | 0.97 | 0.97 |\n",
    "| 8. | XGBoost Model | 96.64 | 0.97 | 0.97 | 0.97 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### __Due to the high accuracy and precision, we are choosing the *Random Forest Classifier* model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section503></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Fit and tune models with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Clasifiers\n",
    "scores1 = cross_val_score(dtmodel, X, y, cv=10, scoring='accuracy') \n",
    "scores1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "scores2 = cross_val_score(rfc, X, y, cv=10, scoring='accuracy') \n",
    "scores2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Model\n",
    "scores3 = cross_val_score(gb_model, X, y, cv=10, scoring='accuracy') \n",
    "scores3.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Model\n",
    "scores4 = cross_val_score(knn, X, y, cv=10, scoring='accuracy') \n",
    "scores4.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also, clearly the _Random Forest Classifier_ is a winner here as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section504></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Declare hyper-parameters to tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "grid_param = {'n_estimators': [300,400,500,600], 'max_depth':[10,15,20], \"min_samples_leaf\": [2,4,6]}\n",
    "rfc_grid = GridSearchCV(estimator=rfc, param_grid=grid_param,cv=5)\n",
    "rfc_grid.fit( X_train , y_train)\n",
    "y_pred_test_hp = rfc_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfc_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = metrics.accuracy_score(y_test, y_pred_test_hp)*100\n",
    "print('Accuracy is :',selected_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfc_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_matrix_employee(y_test, y_pred_test_hp, \"Random Forest Classifier Hyper-Parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "print(metrics.classification_report(y_test,y_pred_test_hp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section505></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_feature_imp=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=2, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_feature_imp.fit( X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(rfc_feature_imp.feature_importances_, index=X_train.columns)\n",
    "feat_importances.nlargest(20).plot(kind='barh')\n",
    "for i, j in sorted(zip(X_train.columns, rfc_feature_imp.feature_importances_)):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above feature importance plot, it seems _satisfaction_ has a major say in determining attrition of employees followed by _number of projects_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section503></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section6></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Predicting the unkown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section601></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Format the unseen data as per our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import the unseen data\n",
    "unseenData = pd.read_csv('https://docs.google.com/spreadsheets/d/1QNeMmV0PiNoCMSivz7ScZ8NCAOUWP_ThmUNikEMFNIY/export?gid=1947002331&format=csv') # if needed index_col=0\n",
    "unseenData.to_csv('unseenData.csv')\n",
    "unseenData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseenData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseenData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseenData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unseenData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unseenData.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unseenData['department'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseenData['department'].replace({'-IT': 'D00-IT'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unseenData['department'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseenData.isnull().sum()[unseenData.isnull().sum() !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = unseenData.isnull().sum()[unseenData.isnull().sum() !=0]\n",
    "missing = pd.DataFrame(missing.reset_index())\n",
    "missing.rename(columns={'index':'features',0:'missing_count'}, inplace = True)\n",
    "missing['missing_count_percentage'] = ((missing['missing_count'])/unseenData.shape[0])*100\n",
    "plt.figure()\n",
    "sns.barplot(y = missing['features'], x = missing['missing_count_percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DataForPred = pd.merge(unseenData, df_employeeDetailsData, how='left', on=\"employee_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DataForPred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DataForPred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DataForPred.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DataForPred.isnull().sum()[df_DataForPred.isnull().sum() !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df_DataForPred.isnull().sum()[df_DataForPred.isnull().sum() !=0]\n",
    "missing = pd.DataFrame(missing.reset_index())\n",
    "missing.rename(columns={'index':'features',0:'missing_count'}, inplace = True)\n",
    "missing['missing_count_percentage'] = ((missing['missing_count'])/df_DataForPred.shape[0])*100\n",
    "plt.figure()\n",
    "sns.barplot(y = missing['features'], x = missing['missing_count_percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DataForPred['department'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DataForPred.department.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing values with dept_name : D00-SS.i.e. the Sales department as most of the employees are from this department\n",
    "df_DataForPred['department'].fillna('D00-SS',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finData = pd.merge(df_DataForPred, df_departmentData, how='left', left_on='department', right_on='dept_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finData.isnull().sum()[df_finData.isnull().sum() !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df_finData.isnull().sum()[df_finData.isnull().sum() !=0]\n",
    "missing = pd.DataFrame(missing.reset_index())\n",
    "missing.rename(columns={'index':'features',0:'missing_count'}, inplace = True)\n",
    "missing['missing_count_percentage'] = ((missing['missing_count'])/df_finData.shape[0])*100\n",
    "plt.figure()\n",
    "sns.barplot(y = missing['features'], x = missing['missing_count_percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary fields\n",
    "df_finData.drop(['filed_complaint','recently_promoted', 'department', 'dept_id', 'dept_head'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finData.isnull().sum()[df_finData.isnull().sum() !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(figsize=(15,10))\n",
    "sns.boxplot(df_finData['last_evaluation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_evaluation has no outliers - Mean should do the job here\n",
    "df_finData['last_evaluation'].fillna(df_finData['last_evaluation'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(figsize=(15,10))\n",
    "sns.boxplot(df_finData['satisfaction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# satisfaction has no outliers - Mean should do the job here\n",
    "df_finData['satisfaction'].fillna(df_finData['satisfaction'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(figsize=(15,10))\n",
    "sns.boxplot(df_finData['tenure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tenure has one outliers - Median should do the job here\n",
    "df_finData['tenure'].fillna(df_finData['tenure'].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finData.isnull().sum()[df_finData.isnull().sum() !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the data to fir our model\n",
    "le = LabelEncoder()\n",
    "df_finData['gender'] = le.fit_transform(df_finData['gender'])\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)\n",
    "df_finData['marital_status'] = le.fit_transform(df_finData['marital_status'])\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)\n",
    "df_finData['salary'] = le.fit_transform(df_finData['salary'])\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)\n",
    "df_finData['dept_name'] = le.fit_transform(df_finData['dept_name'])\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_finData[['avg_monthly_hrs', 'last_evaluation', 'n_projects', 'salary', 'satisfaction', 'tenure', 'age', 'gender', 'marital_status', 'dept_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section602></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Run the model through the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_unseenData = rfc_grid.predict(X)\n",
    "y_pred_unseenData_probability = rfc_grid.predict_proba(X)\n",
    "preds = y_pred_unseenData_probability[:,1]\n",
    "print (y_pred_unseenData.shape, y_pred_unseenData_probability.shape, preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred_unseenData, columns=['status'])\n",
    "y_pred_proba_df = pd.DataFrame(preds, columns=['Probability to Leave'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (y_pred_df.shape, y_pred_proba_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (y_pred_df.columns, y_pred_proba_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimals = 2    \n",
    "y_pred_proba_df['Probability to Leave'] = y_pred_proba_df['Probability to Leave'].apply(lambda x: round(x, decimals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_df.head())\n",
    "print(y_pred_proba_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df.status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = section603></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Publishing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empColumn = df_finData[['employee_id']]\n",
    "empColumn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_publish = pd.concat([empColumn, y_pred_proba_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final_publish.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_publish.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_publish.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_publish.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_publish.to_csv('1003_HR_ProbabilityData.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This csv file will be uploaded to the server. The HR can take a look at the employees who have high probablity of leaving and try to see if they work something in their favour. Majorly, statisfaction, working in multiple projects, gender, etc. need to be taken into account while dealing with the employee. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
